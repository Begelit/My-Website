{% extends 'test_app/base.html' %}

{% block title %}
<title>SWB - Home</title>
{% endblock %}

{% block homeContent %}
<h1 id="test">index page {{text}}</h1>
{% endblock %}

{% block canvasBlock %}
<canvas class="visualizer" width="640" height="100"></canvas>
{% endblock %}

{% block wsScript %}

<!-- ----------- --> <!-- ALTERNATIVE --><!-- ------------ -->
<!-- ------------ --><!-- ----------- --><!-- ------------ -->
<!-- ------------ --><!-- ----------- --><!-- ------------ -->

<script>

    'use strict';

    const heading = document.querySelector("h3");
    heading.textContent = "CLICK HERE TO START";
    //document.body.addEventListener("click", main);
    document.body.addEventListener("click", init);

    const canvas = document.querySelector(".visualizer");
    const canvasCtx = canvas.getContext("2d");

    const intendedWidth = document.querySelector(".wrapper").clientWidth;
    canvas.setAttribute("width", intendedWidth);
    /*
    document.body.addEventListener('click', (element) => {
        init();
        //document.querySelector('#click-to-start').remove();
    }, {once: true});
    */
    const context = new AudioContext({sampleRate: 16000,});

    async function init () {
        if (context.state === 'suspended') {
            await context.resume();
        }
        const micStream = await navigator.mediaDevices.getUserMedia({
            audio:  {
                //channelCount: 1,
                echoCancellation: false,
                autoGainControl: false,
                noiseSuppression: false,
                latency: 0,
            },
        });
        const micSourceNode = await context.createMediaStreamSource(micStream);
        //let track =  micStream.getAudioTracks()[0];
        //await track.applyConstraints({channelCount:1,});
        const recordingProperties = {
            numberOfChannels: micSourceNode.channelCount,
            sampleRate: context.sampleRate,
            bufferLength: Math.round((context.sampleRate/2)/128)*128,
        };
        console.log(recordingProperties);
        
        const recordingNode = await setupRecordingWorkletNode(recordingProperties);
        //const monitorNode = context.createGain();
        //const analyser = context.createAnalyser();
        //analyser.fftsize = 16384;
        console.log(recordingNode);

        micSourceNode
        .connect(recordingNode)
        //.connect(monitorNode)
        .connect(context.destination);
  
        const recordingCallback = handleRecording(recordingNode.port, recordingProperties);

        recordingNode.port.onmessage = (event) => {
            //console.log(event.data.inpgain);
            recordingCallback(event);
            //visualize(dataArray,128,analyser);
        };

        
    } 

    function handleRecording(recording_port, recording_properties){
        let current_bufferLength = 0;
        const recordingEventCallback = async (event) => {
            if(event.data.message === "MAX_BUFFER_LENGTH"){
                const recordingBuffer = context.createBuffer(
                    recording_properties.numberOfChannels,
                    recording_properties.bufferLength,
                    recording_properties.sampleRate,
                );
                for (let i = 0; i < recording_properties.numberOfChannels; i++) {
                    recordingBuffer.copyToChannel(event.data.buffer_array[i], i, 0);
                }
                //const wavUrl = createLinkFromAudioBuffer(recordingBuffer, false);
                //console.log(wavUrl);
                //let wave_array  = _createWaveFileBlobFromAudioBuffer(recordingBuffer, false);
                //let wave_array = audioBufferToWav(recordingBuffer,false);
                console.log(event.data);//recordingBuffer.getChannelData);
                //console.log(wave_array);
            }
            
        };
        return recordingEventCallback;
    }

    /**
     * Creates ScriptProcessor to record and track microphone audio.
     * @param {object} recordingProperties
     * @param {number} properties.numberOfChannels
     * @param {number} properties.sampleRate
     * @return {AudioWorkletNode} Recording node related components for the app.
     */
    async function setupRecordingWorkletNode(recordingProperties) {
        await context.audioWorklet.addModule('static/dj_app/js/recorder_worklet.js');

        const WorkletRecordingNode = new AudioWorkletNode(
            context,
            'recorder_worklet',
            {
                processorOptions: recordingProperties,
            },
        );

        return WorkletRecordingNode;
    }



    function visualize(dataArray,bufferLength,analyser) {
            let WIDTH = canvas.width;
            let HEIGHT = canvas.height;
            //console.log(WIDTH,HEIGHT)

            const visualSetting = "sinewave"//visualSelect.value;
            //console.log(visualSetting);

            if (visualSetting === "sinewave") {
                //analyser.fftSize = 16384;
                //const bufferLength = analyser.fftSize;
                //console.log('-----------',bufferLength);

                // We can use Float32Array instead of Uint8Array if we want higher precision
                // const dataArray = new Float32Array(bufferLength);
                //const dataArray = new Uint8Array(bufferLength);
                //const new_dataArray = new Uint8Array(analyser.frequencyBinCount);
                //console.log(dataArray)

                canvasCtx.clearRect(0, 0, WIDTH, HEIGHT);

                const draw = function () {
                    let drawVisual = requestAnimationFrame(draw);

                    analyser.getByteTimeDomainData(dataArray);
                    //analyser.getFloatFrequencyData(dataArray);
                    //analyser.getByteFrequencyData(new_dataArray)
                    //console.log(dataArray)
                    canvasCtx.fillStyle = "rgb(200, 200, 200)";
                    canvasCtx.fillRect(0, 0, WIDTH, HEIGHT);

                    canvasCtx.lineWidth = 2;
                    canvasCtx.strokeStyle = "rgb(0, 0, 0)";

                    canvasCtx.beginPath();
                    
                    const sliceWidth = (WIDTH * 1.0) / bufferLength;
                    let x = 0;
                    
                    for (let i = 0; i < bufferLength; i++) {
                        //console.log(i)
                        let v = dataArray[i]/ 128.0;
                        let y = (v * HEIGHT) / 2;

                        //new_dataArray[i] = v

                        if (i === 0) {
                            canvasCtx.moveTo(x, y)
                        } else {
                            canvasCtx.lineTo(x, y);
                        }

                        x += sliceWidth;
                    }
                    
                    //console.log(new_dataArray)

                    canvasCtx.lineTo(canvas.width, canvas.height / 2);
                    canvasCtx.stroke();
                }

                draw();
            }
        }

    //const heading = document.querySelector("h3");
    //heading.textContent = "CLICK HERE TO START";
    //document.body.addEventListener("click", init);


    var socket = new WebSocket('ws://localhost:8000/ws/some_url/');
    socket.onmessage = function(event){
        var data = JSON.parse(event.data);
        console.log(data);
        document.querySelector('#test').innerText = data.message;
    }
</script>
{% endblock %}

{% block styleBlock %}
<style>
.wrapper {
    height: 100%;
    max-width: 800px;
    margin: 0 auto;
}
canvas {
    border-top: 1px solid black;
    border-bottom: 1px solid black;
    margin-bottom: -3px;
    box-shadow: 0 -2px 4px rgba(0, 0, 0, 0.7), 0 3px 4px rgba(0, 0, 0, 0.7);
    }
</style>
{% endblock %}